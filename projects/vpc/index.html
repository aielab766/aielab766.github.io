<!DOCTYPE html>
<html>

  <head>
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>AICV Lab - Research</title>
	<meta name="description" content="AICV Lab -- Research">
	<link rel="stylesheet" href="/css/main.css">
	<link rel="canonical" href="/projects/vpc/">
	<link rel="shortcut icon" type ="image/x-icon" href="/images/favicon.ico">
	<link rel="stylesheet" href="/css/all.css" integrity="sha384-DNOHZ68U8hZfKXOrtjWvjxusGo9WQnrNx2sqG0tfsghAvtVlRW3tvkXWZh58N9jp" crossorigin="anonymous">
	<link rel="stylesheet" href="/css/academicons/css/academicons.css">

	
</head>


  <body>

    <div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container-fluid">
	<div class="navbar-header">
	  <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar-collapse-1" aria-expanded="false">
		<span class="sr-only">Toggle navigation</span>
		<span class="icon-bar"></span>
		<span class="icon-bar"></span>
		<span class="icon-bar"></span>
	  </button>
	
    <a class="navbar-brand" href="/">AIE Lab Room 766&423</a>
	</div>

	<div class="collapse navbar-collapse" id="navbar-collapse-1">
	  <ul class="nav navbar-nav navbar-right">
		<li><a href="/">Home</a></li>
		<li><a href="/team">Team</a></li>
		<!-- <li><a href="/research">Research</a></li> -->
		<li><a href="/publications">Publications</a></li>
		<li><a href="/recruitment">Join Us</a></li>
		<li><a href="/contact">Contact</a></li>
	  </ul>
	</div>
	
  </div>
</div>


    <div class="container-fluid">
      <div class="row">
        <div id="gridid" class="col-sm-12">
  <h2 id="video-paragraph-captioning">Video Paragraph Captioning</h2>

<p><img src="/images/respic/vltint_vpc_demo1.gif" alt="" style="width: 100%;  margin: 6px 0px 6px 0; border-radius:0%; box-shadow: 0px 0px 0px" /></p>

<div style="text-align: justify">
  <p>Video paragraph captioning aims to generate a multi-sentence description of an untrimmed video with several temporal event locations in coherent storytelling. 
Following the human perception process, where the scene is effectively understood by decomposing it into visual (e.g. human, animal) and non-visual components (e.g. action, relations) under the mutual influence of vision and language, we first propose a visual-linguistic (VL) feature. In the proposed VL feature, the scene is modeled by three modalities including (i) a global visual environment; (ii) local visual main agents; (iii) linguistic scene elements. We then introduce an autoregressive Transformer-in-Transformer (TinT) to simultaneously capture the semantic coherence of intra- and inter-event contents within a video. Finally, we present a new VL contrastive loss function to guarantee learnt embedding features are matched with the captions semantics. Comprehensive experiments and extensive ablation studies on ActivityNet Captions and YouCookII datasets show that the proposed Visual-Linguistic Transformer-in-Transform (VLTinT) outperforms prior state-of-the-art methods on accuracy and diversity.</p>
</div>

<h3 id="project-members">Project Members</h3>


</div>

      </div>
    </div>

    <div id="footer" class="panel">
  <div class="panel-footer">
	<div class="container-fluid">
	  <div class="row">

			<ul class="nav navbar-nav navbar-right">
				<li><a href="https://xzxy.ahau.edu.cn/">CIAI</a></li>
				<li><a href="https://www.ahau.edu.cn/">AAU</a></li>
				<li><a href="/about">About</a></li>
			</ul>

	  </div>
	</div>
  </div>
</div>

<script src="js/jquery.min.js"></script>
<script src="/js/bootstrap.min.js"></script>


  </body>

</html>
